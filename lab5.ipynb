{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 5, студент Устинов Денис Александрович М8О-406Б-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Выбор начальных условий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Набор данных для задачи классификации\n",
    "\n",
    "Задача: Определение типа активности человека на основе данных с носимых устройств\n",
    "\n",
    "Набор данных: Human Activity Recognition Dataset (HAR) (https://archive.ics.uci.edu/dataset/240/human+activity+recognition+using+smartphones), где данные получены с носимых датчиков и классифицируются в такие категории, как ходьба, сидение, подъем по лестнице и т. д.\n",
    "\n",
    "Обоснование выбора: HAR представляет практическую задачу для классификации с несколькими классами и используется в задачах мониторинга здоровья и физической активности. Задача распознавания активности с помощью KNN полезна и практична, так как такой алгоритм достаточно прост для реализации в системах с носимыми устройствами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Набор данных для задачи регрессии\n",
    "\n",
    "Задача: Прогнозирование уровня выбросов CO2 автомобилей\n",
    "\n",
    "Набор данных: CO2 Emission by Vehicles (https://www.kaggle.com/datasets/debajyotipodder/co2-emission-by-vehicles), включающий параметры автомобиля (тип двигателя, вес, объем двигателя) и уровень выбросов CO2.\n",
    "\n",
    "Обоснование выбора: Прогнозирование выбросов позволяет оценивать экологическое воздействие автомобилей на основе их характеристик, что является важной задачей в экологии и инженерии. Такой набор данных также подходит для KNN, так как близость характеристик автомобилей позволяет строить точные прогнозы выбросов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Выбор метрик качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для задачи классификации:\n",
    "\n",
    "1) Accuracy (Точность): доля правильно классифицированных объектов. Хорошо подходит для задачи с равномерным распределением классов.\n",
    "2) Precision: можно интерпретировать как долю объектов, названных классификатором положительными и при этом действительно являющимися положительными\n",
    "3) Recall: показывает, какую долю объектов положительного класса из всех объектов положительного класса нашел алгоритм.\n",
    "2) F1-score: среднее гармоническое между точностью (precision) и полнотой (recall), особенно полезно, если классы несбалансированы, так как учитывает ложноположительные и ложноотрицательные предсказания.\n",
    "\n",
    "Для задачи регрессии:\n",
    "\n",
    "1) Mean Absolute Error (MAE): средняя абсолютная ошибка, показывает среднее отклонение предсказаний от фактических значений и хорошо интерпретируется.\n",
    "2) Mean Squared Error (MSE): средняя квадратичная ошибка, показывает более крупные ошибки больше чем MAE, что делает его полезным для выявления больших отклонений в прогнозах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Создание бейзлайна и оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Обучить модели из sklearn (для классификации) для выбранных наборов данных и оценить качество моделей (для классификации) по выбранным метрикам на выбранных наборах данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предобработаем данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение и очистка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list()\n",
    "with open('UCI HAR Dataset/features.txt') as f:\n",
    "    features = [line.split()[1] for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дубликаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen = set()\n",
    "uniq_features = []\n",
    "for idx, x in enumerate(features):\n",
    "    if x not in seen:\n",
    "        uniq_features.append(x)\n",
    "        seen.add(x)\n",
    "    elif x + 'n' not in seen:\n",
    "        uniq_features.append(x + 'n')\n",
    "        seen.add(x + 'n')\n",
    "    else:\n",
    "        uniq_features.append(x + 'nn')\n",
    "        seen.add(x + 'nn')\n",
    "len(uniq_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим обучающую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(arg):\n",
    "    if arg == 1: \n",
    "        return 'WALKING'\n",
    "    if arg == 2:\n",
    "        return 'WALKING_UPSTAIRS'\n",
    "    if arg == 3:\n",
    "        return 'WALKING_DOWNSTAIRS'\n",
    "    if arg == 4:\n",
    "        return 'SITTING'\n",
    "    if arg == 5:\n",
    "        return 'STANDING'\n",
    "    if arg == 6:\n",
    "        return 'LAYING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_37013/3107987138.py:1: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  X_train = pd.read_csv('UCI HAR Dataset/train/X_train.txt', delim_whitespace=True, header=None, names=uniq_features)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "      <th>ActivityName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>0.278665</td>\n",
       "      <td>-0.03279</td>\n",
       "      <td>-0.109089</td>\n",
       "      <td>-0.965966</td>\n",
       "      <td>-0.873944</td>\n",
       "      <td>-0.941454</td>\n",
       "      <td>-0.985601</td>\n",
       "      <td>-0.89831</td>\n",
       "      <td>-0.947819</td>\n",
       "      <td>-0.814786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00396</td>\n",
       "      <td>0.463115</td>\n",
       "      <td>-0.228913</td>\n",
       "      <td>0.073795</td>\n",
       "      <td>-0.903283</td>\n",
       "      <td>0.090578</td>\n",
       "      <td>-0.053438</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  \\\n",
       "870           0.278665           -0.03279          -0.109089   \n",
       "\n",
       "     tBodyAcc-std()-X  tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  \\\n",
       "870         -0.965966         -0.873944         -0.941454         -0.985601   \n",
       "\n",
       "     tBodyAcc-mad()-Y  tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  \\\n",
       "870          -0.89831         -0.947819         -0.814786  ...   \n",
       "\n",
       "     angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "870                     -0.00396                              0.463115   \n",
       "\n",
       "     angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "870                         -0.228913                              0.073795   \n",
       "\n",
       "     angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  \\\n",
       "870             -0.903283              0.090578             -0.053438   \n",
       "\n",
       "     subject  Activity  ActivityName  \n",
       "870        5         5      STANDING  \n",
       "\n",
       "[1 rows x 564 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv('UCI HAR Dataset/train/X_train.txt', delim_whitespace=True, header=None, names=uniq_features)\n",
    "X_train['subject'] = pd.read_csv('UCI HAR Dataset/train/subject_train.txt', header=None)\n",
    "\n",
    "y_train = pd.read_csv('UCI HAR Dataset/train/y_train.txt', names=['Activity'])\n",
    "y_train_labels = y_train.map(map_func)\n",
    "\n",
    "train = X_train\n",
    "train['Activity'] = y_train\n",
    "train['ActivityName'] = y_train_labels\n",
    "train.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_37013/2096605161.py:1: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  X_test = pd.read_csv('UCI HAR Dataset/test/X_test.txt', delim_whitespace=True, header=None, names=uniq_features)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "      <th>ActivityName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>0.275777</td>\n",
       "      <td>-0.016968</td>\n",
       "      <td>-0.104138</td>\n",
       "      <td>-0.992826</td>\n",
       "      <td>-0.996707</td>\n",
       "      <td>-0.994813</td>\n",
       "      <td>-0.993326</td>\n",
       "      <td>-0.995837</td>\n",
       "      <td>-0.99381</td>\n",
       "      <td>-0.93692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.21192</td>\n",
       "      <td>-0.03751</td>\n",
       "      <td>0.319525</td>\n",
       "      <td>-0.076225</td>\n",
       "      <td>0.558448</td>\n",
       "      <td>-0.794873</td>\n",
       "      <td>-0.199187</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>LAYING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  \\\n",
       "960           0.275777          -0.016968          -0.104138   \n",
       "\n",
       "     tBodyAcc-std()-X  tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  \\\n",
       "960         -0.992826         -0.996707         -0.994813         -0.993326   \n",
       "\n",
       "     tBodyAcc-mad()-Y  tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  \\\n",
       "960         -0.995837          -0.99381          -0.93692  ...   \n",
       "\n",
       "     angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "960                     -0.21192                              -0.03751   \n",
       "\n",
       "     angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "960                          0.319525                             -0.076225   \n",
       "\n",
       "     angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  \\\n",
       "960              0.558448             -0.794873             -0.199187   \n",
       "\n",
       "     subject  Activity  ActivityName  \n",
       "960       10         6        LAYING  \n",
       "\n",
       "[1 rows x 564 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv('UCI HAR Dataset/test/X_test.txt', delim_whitespace=True, header=None, names=uniq_features)\n",
    "X_test['subject'] = pd.read_csv('UCI HAR Dataset/test/subject_test.txt', header=None)\n",
    "\n",
    "y_test = pd.read_csv('UCI HAR Dataset/test/y_test.txt', names=['Activity'])\n",
    "y_test_labels = y_test.map(map_func)\n",
    "\n",
    "test = X_test\n",
    "test['Activity'] = y_test\n",
    "test['ActivityName'] = y_test_labels\n",
    "test.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поменяем названия функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z',\n",
       "       'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z',\n",
       "       'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z',\n",
       "       'tBodyAcc-max()-X',\n",
       "       ...\n",
       "       'angle(tBodyAccMean,gravity)', 'angle(tBodyAccJerkMean),gravityMean)',\n",
       "       'angle(tBodyGyroMean,gravityMean)',\n",
       "       'angle(tBodyGyroJerkMean,gravityMean)', 'angle(X,gravityMean)',\n",
       "       'angle(Y,gravityMean)', 'angle(Z,gravityMean)', 'subject', 'Activity',\n",
       "       'ActivityName'],\n",
       "      dtype='object', length=564)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = train.columns\n",
    "\n",
    "columns = columns.str.replace('[()]','')\n",
    "columns = columns.str.replace('[-]', '')\n",
    "columns = columns.str.replace('[,]','')\n",
    "\n",
    "train.columns = columns\n",
    "test.columns = columns\n",
    "\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним выборки как csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/train.csv', index=False)\n",
    "test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    data = sklearn.utils.shuffle(data)\n",
    "    \n",
    "    X_data = data.drop(['subject', 'Activity', 'ActivityName'], axis=1)\n",
    "    y_data = data.ActivityName\n",
    "    \n",
    "    return np.array(X_data), np.array(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Обучить модели с градиентным бустингом из sklearn (для классификации) для выбранных наборов данных и оценить качество моделей (для классификации) по выбранным метрикам на выбранных наборах данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для задачи классификации:\n",
      "Accuracy: 0.9237\n",
      "Recall: 0.9237\n",
      "Precision: 0.9241\n",
      "F1 Score: 0.9237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "train_X, train_y = read_data('data/train.csv')\n",
    "test_X, test_y = read_data('data/test.csv')\n",
    "\n",
    "np.random.seed(42)\n",
    "train_sample_indices = np.random.choice(train_X.shape[0], size=3000, replace=False)\n",
    "train_X = train_X[train_sample_indices]\n",
    "train_y = train_y[train_sample_indices]\n",
    "\n",
    "classifier = GradientBoostingClassifier()\n",
    "\n",
    "classifier.fit(train_X, train_y)\n",
    "\n",
    "y_pred = classifier.predict(test_X)\n",
    "\n",
    "accuracy = accuracy_score(test_y, y_pred)\n",
    "recall = recall_score(test_y, y_pred, average='weighted')\n",
    "precision = precision_score(test_y, y_pred, average='weighted')\n",
    "f1 = f1_score(test_y, y_pred, average='weighted')\n",
    "\n",
    "print(\"Результаты для задачи классификации:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Обучить модели с градиентным бустингом из sklearn (для регрессии) для выбранных наборов данных и оценить качество моделей (для регрессии) по выбранным метрикам на выбранных наборах данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для задачи регрессии:\n",
      "MAE: 5.13\n",
      "MSE: 94.89\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]]\n",
    "y = data['CO2 Emissions(g/km)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "regressor = GradientBoostingRegressor()\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = regressor.predict(X_test)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"Результаты для задачи регрессии:\")\n",
    "print(f\"MAE: {test_mae:.2f}\")\n",
    "print(f\"MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Улучшение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Формирование новых признаков\n",
    "    - Комбинация признаков: Например, для задачи регрессии по CO2 выбросам можно создать новый признак, комбинирующий объем двигателя и расход топлива, или вывести дополнительные признаки, отражающие производительность.\n",
    "\n",
    "2.  Подбор гиперпараметров\n",
    "    - n_estimators: Количество деревьев в ансамбле.\n",
    "    - max_depth: Максимальная глубина деревьев.\n",
    "    - learning_rate: Скорость обучения.\n",
    "\n",
    "3. Методы оптимизации\n",
    "    - Grid Search: Провести поиск по сетке (Grid Search) для поиска оптимальных параметров модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор гиперпараметров и Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time= 1.6min\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time= 1.6min\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time= 1.6min\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time= 3.1min\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time= 3.1min\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time= 3.1min\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time= 2.5min\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time= 2.5min\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=150; total time= 4.7min\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=150; total time= 4.7min\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time= 2.5min\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=150; total time= 4.6min\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time= 4.9min\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time= 5.0min\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time= 4.9min\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time= 3.4min\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time= 3.4min\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=150; total time= 7.3min\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time= 3.4min\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=150; total time= 7.3min\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=150; total time= 7.3min\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time= 1.5min\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time= 1.5min\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time= 6.6min\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time= 1.5min\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time= 6.6min\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time= 6.6min\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time= 3.0min\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=150; total time= 8.0min\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time= 3.1min\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time= 3.0min\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=150; total time= 7.5min\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=150; total time= 7.7min\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=150; total time= 4.3min\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=150; total time= 4.3min\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time= 2.5min\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time= 2.5min\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time= 2.5min\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=150; total time= 4.4min\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time= 4.5min\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time= 4.5min\n",
      "[CV] END ....learning_rate=0.2, max_depth=7, n_estimators=50; total time= 3.3min\n",
      "[CV] END ....learning_rate=0.2, max_depth=7, n_estimators=50; total time= 3.3min\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time= 4.4min\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=150; total time= 5.3min\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=150; total time= 5.4min\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=150; total time= 5.2min\n",
      "[CV] END ....learning_rate=0.2, max_depth=7, n_estimators=50; total time= 3.2min\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=100; total time= 4.0min\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=100; total time= 3.8min\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=100; total time= 3.5min\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=150; total time= 3.8min\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=150; total time= 3.3min\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=150; total time= 3.5min\n",
      "Лучшие параметры для классификации:\n",
      "{'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Результаты после оптимизации гиперпараметров для задачи классификации:\n",
      "Accuracy: 0.9270\n",
      "Recall: 0.9270\n",
      "Precision: 0.9280\n",
      "F1 Score: 0.9270\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "train_X, train_y = read_data('data/train.csv')\n",
    "test_X, test_y = read_data('data/test.csv')\n",
    "\n",
    "np.random.seed(42)\n",
    "train_sample_indices = np.random.choice(train_X.shape[0], size=3000, replace=False)\n",
    "train_X = train_X[train_sample_indices]\n",
    "train_y = train_y[train_sample_indices]\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.2]\n",
    "}\n",
    "\n",
    "classifier = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='f1_weighted', cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(train_X, train_y)\n",
    "\n",
    "print(\"Лучшие параметры для классификации:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_classifier = grid_search.best_estimator_\n",
    "y_pred = best_classifier.predict(test_X)\n",
    "\n",
    "accuracy = accuracy_score(test_y, y_pred)\n",
    "recall = recall_score(test_y, y_pred, average='weighted')\n",
    "precision = precision_score(test_y, y_pred, average='weighted')\n",
    "f1 = f1_score(test_y, y_pred, average='weighted')\n",
    "\n",
    "print(\"Результаты после оптимизации гиперпараметров для задачи классификации:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=150; total time=   0.3s\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time=   0.3s\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time=   0.3s\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=150; total time=   0.6s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=150; total time=   0.6s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=150; total time=   0.6s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.5s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.5s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.6s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=150; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=150; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=150; total time=   0.8s\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=150; total time=   0.8s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=150; total time=   0.8s\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=150; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END ....learning_rate=0.2, max_depth=7, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END ....learning_rate=0.2, max_depth=7, n_estimators=50; total time=   0.3s\n",
      "[CV] END ....learning_rate=0.2, max_depth=7, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=150; total time=   0.5s\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=100; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=100; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=100; total time=   0.5s\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=150; total time=   0.7s\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=150; total time=   0.6s\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=150; total time=   0.6s\n",
      "Лучшие параметры для регрессии:\n",
      "{'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 150}\n",
      "Результаты после оптимизации гиперпараметров для задачи регрессии:\n",
      "MAE: 3.09\n",
      "MSE: 46.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]]\n",
    "y = data['CO2 Emissions(g/km)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.2]\n",
    "}\n",
    "\n",
    "regressor = GradientBoostingRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Лучшие параметры для регрессии:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_regressor = grid_search.best_estimator_\n",
    "y_pred_test = best_regressor.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"Результаты после оптимизации гиперпараметров для задачи регрессии:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формирование новых признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для задачи регрессии:\n",
      "MAE: 4.91\n",
      "MSE: 82.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]]\n",
    "y = data['CO2 Emissions(g/km)']\n",
    "\n",
    "X_train_new_field, X_test_new_field, y_train_new_field, y_test_new_field = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_new_field['Engine_Fuel'] = X_train_new_field['Engine Size(L)'] * X_train_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "X_test_new_field['Engine_Fuel'] = X_test_new_field['Engine Size(L)'] * X_test_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_new_field)\n",
    "X_test_poly = poly.transform(X_test_new_field)\n",
    "\n",
    "dt_regressor = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "dt_regressor.fit(X_train_poly, y_train_new_field)\n",
    "\n",
    "y_pred_test = dt_regressor.predict(X_test_poly)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test_new_field, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test_new_field, y_pred_test)\n",
    "\n",
    "print(\"Результаты для задачи регрессии:\")\n",
    "print(f\"MAE: {test_mae:.2f}\")\n",
    "print(f\"MSE: {test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Окончательный улучшенный бейзлайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие параметры для классификации:\n",
    "{'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для задачи классификации:\n",
      "Accuracy: 0.9376\n",
      "Recall: 0.9376\n",
      "Precision: 0.9388\n",
      "F1 Score: 0.9374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "train_X, train_y = read_data('data/train.csv')\n",
    "test_X, test_y = read_data('data/test.csv')\n",
    "\n",
    "np.random.seed(42)\n",
    "train_sample_indices = np.random.choice(train_X.shape[0], size=3000, replace=False)\n",
    "train_X = train_X[train_sample_indices]\n",
    "train_y = train_y[train_sample_indices]\n",
    "\n",
    "classifier = GradientBoostingClassifier(max_depth= 3, learning_rate = 0.2, max_features='log2', min_samples_leaf=1, min_samples_split=2, n_estimators=150)\n",
    "\n",
    "classifier.fit(train_X, train_y)\n",
    "\n",
    "y_pred = classifier.predict(test_X)\n",
    "\n",
    "accuracy = accuracy_score(test_y, y_pred)\n",
    "recall = recall_score(test_y, y_pred, average='weighted')\n",
    "precision = precision_score(test_y, y_pred, average='weighted')\n",
    "f1 = f1_score(test_y, y_pred, average='weighted')\n",
    "\n",
    "print(\"Результаты для задачи классификации:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие параметры для регрессии:\n",
    "{'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для задачи регрессии:\n",
      "MAE: 3.20\n",
      "MSE: 38.18\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]]\n",
    "y = data['CO2 Emissions(g/km)']\n",
    "\n",
    "X_train_new_field, X_test_new_field, y_train_new_field, y_test_new_field = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_new_field['Engine_Fuel'] = X_train_new_field['Engine Size(L)'] * X_train_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "X_test_new_field['Engine_Fuel'] = X_test_new_field['Engine Size(L)'] * X_test_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_new_field)\n",
    "X_test_poly = poly.transform(X_test_new_field)\n",
    "\n",
    "dt_regressor = GradientBoostingRegressor(max_depth= 5, learning_rate=0.2, max_features= 'sqrt', min_samples_leaf= 1, min_samples_split= 2, n_estimators= 150)\n",
    "\n",
    "dt_regressor.fit(X_train_poly, y_train_new_field)\n",
    "\n",
    "y_pred_test = dt_regressor.predict(X_test_poly)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test_new_field, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test_new_field, y_pred_test)\n",
    "\n",
    "print(\"Результаты для задачи регрессии:\")\n",
    "print(f\"MAE: {test_mae:.2f}\")\n",
    "print(f\"MSE: {test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После проведения оптимизации гиперпараметров для моделей градиентного бустинга из библиотеки sklearn удалось существенно улучшить их результаты как в задаче классификации, так и в задаче регрессии. В задаче классификации наблюдается заметное повышение всех основных метрик: точности, полноты, точности предсказаний и F1-меры. Например, точность модели увеличилась с 0.9237 до 0.9376, что свидетельствует о лучшем распознавании классов. В задаче регрессии также достигнуты значительные улучшения. Средняя абсолютная ошибка (MAE) снизилась с 5.13 до 3.20, а среднеквадратичная ошибка (MSE) уменьшилась с 94.89 до 38.18, что говорит о более точных предсказаниях целевой переменной. Эти результаты подтверждают эффективность подхода по подбору оптимальных гиперпараметров для повышения производительности моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Имплементация алгоритма машинного обучения "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация градиентного бустинга для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "\n",
    "class GradientBoostingClassifierCustom:\n",
    "    def __init__(self, n_estimators=100, max_depth=3, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.initial_prediction = None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.models = {cls: [] for cls in self.classes_}\n",
    "        self.initial_prediction = {cls: np.log(np.sum(y == cls) / len(y)) for cls in self.classes_}\n",
    "        y_one_hot = np.array([[1 if yi == cls else 0 for cls in self.classes_] for yi in y])\n",
    "\n",
    "        for cls_idx, cls in enumerate(self.classes_):\n",
    "            residuals = y_one_hot[:, cls_idx] - self._sigmoid(self.initial_prediction[cls])\n",
    "            for _ in range(self.n_estimators):\n",
    "                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "                tree.fit(X, residuals)\n",
    "                predictions = tree.predict(X)\n",
    "                residuals -= self.learning_rate * predictions\n",
    "                self.models[cls].append(tree)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X)\n",
    "        logits = {cls: np.full(X.shape[0], self.initial_prediction[cls]) for cls in self.classes_}\n",
    "        for cls in self.classes_:\n",
    "            for tree in self.models[cls]:\n",
    "                logits[cls] += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        exp_logits = np.exp(np.array(list(logits.values())).T)\n",
    "        probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.array(self.classes_)[np.argmax(proba, axis=1)]\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(logits):\n",
    "        return 1 / (1 + np.exp(-logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация градиентного бустинга для регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "\n",
    "class GradientBoostingRegressorCustom:\n",
    "    def __init__(self, n_estimators=100, max_depth=3, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.initial_prediction = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        self.models = []\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        residuals = y - self.initial_prediction\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            predictions = tree.predict(X)\n",
    "            residuals -= self.learning_rate * predictions\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        predictions = np.full(X.shape[0], self.initial_prediction)\n",
    "        for tree in self.models:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей на выбранных датасетах и вывод метрик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для задачи классификации:\n",
      "Accuracy: 0.8297\n",
      "Recall: 0.8297\n",
      "Precision: 0.8540\n",
      "F1 Score: 0.8271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train_X, train_y = read_data('data/train.csv')\n",
    "test_X, test_y = read_data('data/test.csv')\n",
    "\n",
    "np.random.seed(42)\n",
    "train_sample_indices = np.random.choice(train_X.shape[0], size=3000, replace=False)\n",
    "train_X = train_X[train_sample_indices]\n",
    "train_y = train_y[train_sample_indices]\n",
    "\n",
    "gb_classifier = GradientBoostingClassifierCustom(\n",
    "    n_estimators=10, max_depth=3, learning_rate=0.1\n",
    ")\n",
    "gb_classifier.fit(train_X, train_y)\n",
    "y_pred = gb_classifier.predict(test_X)\n",
    "\n",
    "accuracy = accuracy_score(test_y, y_pred)\n",
    "recall = recall_score(test_y, y_pred, average='weighted')\n",
    "precision = precision_score(test_y, y_pred, average='weighted')\n",
    "f1 = f1_score(test_y, y_pred, average='weighted')\n",
    "\n",
    "print(\"Результаты для задачи классификации:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для задачи регрессии:\n",
      "MAE: 8.26\n",
      "MSE: 163.69\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]]\n",
    "y = data['CO2 Emissions(g/km)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "gb_regressor = GradientBoostingRegressorCustom(\n",
    "    n_estimators=20, max_depth=5, learning_rate=0.1\n",
    ")\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "y_pred_test = gb_regressor.predict(X_test)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"Результаты для задачи регрессии:\")\n",
    "print(f\"MAE: {test_mae:.2f}\")\n",
    "print(f\"MSE: {test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение результатов с п.2. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение результатов обучения моделей из sklearn и собственных имплементаций демонстрирует существенное превосходство библиотечных решений по всем метрикам для задач классификации и регрессии. Для классификации модели из sklearn обеспечивают значительно более высокие значения Accuracy, Recall, Precision и F1 Score, что указывает на их более точное и устойчивое разделение классов. В то же время самописная модель заметно уступает по точности и качеству предсказаний, хотя её Precision остаётся относительно высоким, что может свидетельствовать о склонности к снижению ложных срабатываний, но в ущерб общему охвату (Recall).  \n",
    "\n",
    "Для регрессии модели из sklearn также демонстрируют более точные предсказания, обеспечивая значительно меньшие значения MAE и MSE. Это указывает на способность библиотечной реализации более эффективно минимизировать отклонения предсказаний от истинных значений. Самописная модель, напротив, показывает большее среднее абсолютное и квадратичное отклонение, что говорит о её меньшей эффективности в прогнозировании.  \n",
    "\n",
    "Собственные реализации, хотя и демонстрируют базовую функциональность, существенно уступают в производительности и требуют дальнейшего улучшения для достижения конкурентных результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Добавление нового признака (для задачи регрессии)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 8.15, MSE: 160.08\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]]\n",
    "y = data['CO2 Emissions(g/km)']\n",
    "\n",
    "X_train_new_field, X_test_new_field, y_train_new_field, y_test_new_field = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_new_field['Engine_Fuel'] = X_train_new_field['Engine Size(L)'] * X_train_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "X_test_new_field['Engine_Fuel'] = X_test_new_field['Engine Size(L)'] * X_test_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "\n",
    "rf_regressor = GradientBoostingRegressorCustom(\n",
    "    n_estimators=20, max_depth=5, learning_rate=0.1\n",
    ")\n",
    "rf_regressor.fit(X_train_new_field, y_train_new_field)\n",
    "y_pred_test = rf_regressor.predict(X_test_new_field)\n",
    "\n",
    "mae = mean_absolute_error(y_test_new_field, y_pred_test)\n",
    "mse = mean_squared_error(y_test_new_field, y_pred_test)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}, MSE: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Оптимизация гиперпараметров и Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Задача классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing params: n_estimators=10, max_depth=3, learning_rate=0.01\n",
      "Testing params: n_estimators=10, max_depth=3, learning_rate=0.1\n",
      "Testing params: n_estimators=10, max_depth=3, learning_rate=0.2\n",
      "Testing params: n_estimators=10, max_depth=5, learning_rate=0.01\n",
      "Testing params: n_estimators=10, max_depth=5, learning_rate=0.1\n",
      "Testing params: n_estimators=10, max_depth=5, learning_rate=0.2\n",
      "Testing params: n_estimators=10, max_depth=7, learning_rate=0.01\n",
      "Testing params: n_estimators=10, max_depth=7, learning_rate=0.1\n",
      "Testing params: n_estimators=10, max_depth=7, learning_rate=0.2\n",
      "Testing params: n_estimators=50, max_depth=3, learning_rate=0.01\n",
      "Testing params: n_estimators=50, max_depth=3, learning_rate=0.1\n",
      "Testing params: n_estimators=50, max_depth=3, learning_rate=0.2\n",
      "Testing params: n_estimators=50, max_depth=5, learning_rate=0.01\n",
      "Testing params: n_estimators=50, max_depth=5, learning_rate=0.1\n",
      "Testing params: n_estimators=50, max_depth=5, learning_rate=0.2\n",
      "Testing params: n_estimators=50, max_depth=7, learning_rate=0.01\n",
      "Testing params: n_estimators=50, max_depth=7, learning_rate=0.1\n",
      "Testing params: n_estimators=50, max_depth=7, learning_rate=0.2\n",
      "Testing params: n_estimators=100, max_depth=3, learning_rate=0.01\n",
      "Testing params: n_estimators=100, max_depth=3, learning_rate=0.1\n",
      "Testing params: n_estimators=100, max_depth=3, learning_rate=0.2\n",
      "Testing params: n_estimators=100, max_depth=5, learning_rate=0.01\n",
      "Testing params: n_estimators=100, max_depth=5, learning_rate=0.1\n",
      "Testing params: n_estimators=100, max_depth=5, learning_rate=0.2\n",
      "Testing params: n_estimators=100, max_depth=7, learning_rate=0.01\n",
      "Testing params: n_estimators=100, max_depth=7, learning_rate=0.1\n",
      "Testing params: n_estimators=100, max_depth=7, learning_rate=0.2\n",
      "Best params for classification: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.2}\n",
      "Best F1 Score: 0.8910\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "def grid_search_classification(X_train, y_train, X_test, y_test, param_grid):\n",
    "    best_params = None\n",
    "    best_score = -np.inf\n",
    "\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for learning_rate in param_grid['learning_rate']:\n",
    "                print(f\"Testing params: n_estimators={n_estimators}, max_depth={max_depth}, learning_rate={learning_rate}\")\n",
    "                model = GradientBoostingClassifierCustom(\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth,\n",
    "                    learning_rate=learning_rate\n",
    "                )\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                if f1 > best_score:\n",
    "                    best_score = f1\n",
    "                    best_params = {\n",
    "                        'n_estimators': n_estimators,\n",
    "                        'max_depth': max_depth,\n",
    "                        'learning_rate': learning_rate\n",
    "                    }\n",
    "\n",
    "    print(f\"Best params for classification: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score:.4f}\")\n",
    "    return best_params\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "train_X, train_y = read_data('data/train.csv')\n",
    "test_X, test_y = read_data('data/test.csv')\n",
    "\n",
    "np.random.seed(42)\n",
    "train_sample_indices = np.random.choice(train_X.shape[0], size=3000, replace=False)\n",
    "train_X = train_X[train_sample_indices]\n",
    "train_y = train_y[train_sample_indices]\n",
    "\n",
    "best_params_classification = grid_search_classification(train_X, train_y, test_X, test_y, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Задача регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing params: n_estimators=10, max_depth=3, learning_rate=0.01\n",
      "Testing params: n_estimators=10, max_depth=3, learning_rate=0.1\n",
      "Testing params: n_estimators=10, max_depth=3, learning_rate=0.2\n",
      "Testing params: n_estimators=10, max_depth=5, learning_rate=0.01\n",
      "Testing params: n_estimators=10, max_depth=5, learning_rate=0.1\n",
      "Testing params: n_estimators=10, max_depth=5, learning_rate=0.2\n",
      "Testing params: n_estimators=10, max_depth=7, learning_rate=0.01\n",
      "Testing params: n_estimators=10, max_depth=7, learning_rate=0.1\n",
      "Testing params: n_estimators=10, max_depth=7, learning_rate=0.2\n",
      "Testing params: n_estimators=50, max_depth=3, learning_rate=0.01\n",
      "Testing params: n_estimators=50, max_depth=3, learning_rate=0.1\n",
      "Testing params: n_estimators=50, max_depth=3, learning_rate=0.2\n",
      "Testing params: n_estimators=50, max_depth=5, learning_rate=0.01\n",
      "Testing params: n_estimators=50, max_depth=5, learning_rate=0.1\n",
      "Testing params: n_estimators=50, max_depth=5, learning_rate=0.2\n",
      "Testing params: n_estimators=50, max_depth=7, learning_rate=0.01\n",
      "Testing params: n_estimators=50, max_depth=7, learning_rate=0.1\n",
      "Testing params: n_estimators=50, max_depth=7, learning_rate=0.2\n",
      "Testing params: n_estimators=100, max_depth=3, learning_rate=0.01\n",
      "Testing params: n_estimators=100, max_depth=3, learning_rate=0.1\n",
      "Testing params: n_estimators=100, max_depth=3, learning_rate=0.2\n",
      "Testing params: n_estimators=100, max_depth=5, learning_rate=0.01\n",
      "Testing params: n_estimators=100, max_depth=5, learning_rate=0.1\n",
      "Testing params: n_estimators=100, max_depth=5, learning_rate=0.2\n",
      "Testing params: n_estimators=100, max_depth=7, learning_rate=0.01\n",
      "Testing params: n_estimators=100, max_depth=7, learning_rate=0.1\n",
      "Testing params: n_estimators=100, max_depth=7, learning_rate=0.2\n",
      "Best params for regression: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.2}\n",
      "Best MSE: 50.1052\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def grid_search_regression(X_train, y_train, X_test, y_test, param_grid):\n",
    "    best_params = None\n",
    "    best_score = np.inf\n",
    "\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for learning_rate in param_grid['learning_rate']:\n",
    "                print(f\"Testing params: n_estimators={n_estimators}, max_depth={max_depth}, learning_rate={learning_rate}\")\n",
    "                model = GradientBoostingRegressorCustom(\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth,\n",
    "                    learning_rate=learning_rate\n",
    "                )\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                if mse < best_score:\n",
    "                    best_score = mse\n",
    "                    best_params = {\n",
    "                        'n_estimators': n_estimators,\n",
    "                        'max_depth': max_depth,\n",
    "                        'learning_rate': learning_rate\n",
    "                    }\n",
    "\n",
    "    print(f\"Best params for regression: {best_params}\")\n",
    "    print(f\"Best MSE: {best_score:.4f}\")\n",
    "    return best_params\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]].values\n",
    "y = data['CO2 Emissions(g/km)'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "best_params_regression = grid_search_regression(X_train, y_train, X_test, y_test, param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Окончательный улучшенный бейзлайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие параметры для классификации: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9013, Recall: 0.9013, Precision: 0.9089, F1 Score: 0.9007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "train_X, train_y = read_data('data/train.csv')\n",
    "test_X, test_y = read_data('data/test.csv')\n",
    "\n",
    "np.random.seed(42)\n",
    "train_sample_indices = np.random.choice(train_X.shape[0], size=3000, replace=False)\n",
    "train_X = train_X[train_sample_indices]\n",
    "train_y = train_y[train_sample_indices]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_y = label_encoder.fit_transform(train_y)\n",
    "test_y = label_encoder.transform(test_y)\n",
    "\n",
    "rf_classifier = GradientBoostingClassifierCustom(n_estimators=100, max_depth=3, learning_rate=0.2)\n",
    "\n",
    "rf_classifier.fit(train_X, train_y)\n",
    "y_pred = rf_classifier.predict(test_X)\n",
    "\n",
    "accuracy = accuracy_score(test_y, y_pred)\n",
    "recall = recall_score(test_y, y_pred, average='weighted')\n",
    "precision = precision_score(test_y, y_pred, average='weighted')\n",
    "f1 = f1_score(test_y, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие параметры для регрессии: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.30, MSE: 52.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]]\n",
    "y = data['CO2 Emissions(g/km)']\n",
    "\n",
    "X_train_new_field, X_test_new_field, y_train_new_field, y_test_new_field = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_new_field['Engine_Fuel'] = X_train_new_field['Engine Size(L)'] * X_train_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "X_test_new_field['Engine_Fuel'] = X_test_new_field['Engine Size(L)'] * X_test_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "\n",
    "X_train_new_field = X_train_new_field.values\n",
    "X_test_new_field = X_test_new_field.values\n",
    "y_train_new_field = y_train_new_field.values\n",
    "y_test_new_field = y_test_new_field.values\n",
    "\n",
    "rf_regressor = GradientBoostingRegressorCustom(n_estimators=100, max_depth=5, learning_rate=0.2)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "y_pred_test = rf_regressor.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}, MSE: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение метрик улучшенных моделей из sklearn и самописных моделей демонстрирует, что модели из sklearn после оптимизации гиперпараметров показывают более высокую точность и лучшее качество как в задаче классификации, так и в задаче регрессии. \n",
    "\n",
    "В задаче классификации улучшенные модели из sklearn достигли Accuracy 93.76%, что превосходит показатель самописной модели с её Accuracy 90.13%. Аналогичная тенденция наблюдается для метрик Recall, Precision и F1 Score, которые для моделей sklearn остаются выше. Это связано с высокой оптимизацией и эффективностью алгоритмов sklearn, а также с их точной настройкой через Grid Search. В свою очередь, самописная модель также показала приемлемое качество, особенно учитывая её простоту и кастомную реализацию.\n",
    "\n",
    "В задаче регрессии самописная модель продемонстрировала MAE 3.30 и MSE 52.01, что близко к результатам моделей из sklearn (MAE 3.20 и MSE 38.18). Это указывает на то, что собственная реализация алгоритма градиентного бустинга справляется с задачей регрессии достаточно хорошо, хотя модели sklearn по-прежнему остаются более точными благодаря их внутренним оптимизациям и точному управлению регуляризацией.\n",
    "\n",
    "Таким образом, можно сделать вывод, что модели из sklearn, особенно после оптимизации гиперпараметров, показывают более высокую производительность. Однако результаты самописных моделей свидетельствуют о корректной реализации алгоритма и его способности решать задачи с приемлемой точностью."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimedia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
