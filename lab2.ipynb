{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 2, студент Устинов Денис Александрович М8О-406Б-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Выбор начальных условий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Набор данных для задачи классификации\n",
    "\n",
    "Задача: Определение типа активности человека на основе данных с носимых устройств\n",
    "\n",
    "Набор данных: Human Activity Recognition Dataset (HAR) (https://archive.ics.uci.edu/dataset/240/human+activity+recognition+using+smartphones), где данные получены с носимых датчиков и классифицируются в такие категории, как ходьба, сидение, подъем по лестнице и т. д.\n",
    "\n",
    "Обоснование выбора: HAR представляет практическую задачу для классификации с несколькими классами и используется в задачах мониторинга здоровья и физической активности. Задача распознавания активности с помощью KNN полезна и практична, так как такой алгоритм достаточно прост для реализации в системах с носимыми устройствами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Набор данных для задачи регрессии\n",
    "\n",
    "Задача: Прогнозирование уровня выбросов CO2 автомобилей\n",
    "\n",
    "Набор данных: CO2 Emission by Vehicles (https://www.kaggle.com/datasets/debajyotipodder/co2-emission-by-vehicles), включающий параметры автомобиля (тип двигателя, вес, объем двигателя) и уровень выбросов CO2.\n",
    "\n",
    "Обоснование выбора: Прогнозирование выбросов позволяет оценивать экологическое воздействие автомобилей на основе их характеристик, что является важной задачей в экологии и инженерии. Такой набор данных также подходит для KNN, так как близость характеристик автомобилей позволяет строить точные прогнозы выбросов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Выбор метрик качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для задачи классификации:\n",
    "\n",
    "1) Accuracy (Точность): доля правильно классифицированных объектов. Хорошо подходит для задачи с равномерным распределением классов.\n",
    "2) Precision: можно интерпретировать как долю объектов, названных классификатором положительными и при этом действительно являющимися положительными\n",
    "3) Recall: показывает, какую долю объектов положительного класса из всех объектов положительного класса нашел алгоритм.\n",
    "2) F1-score: среднее гармоническое между точностью (precision) и полнотой (recall), особенно полезно, если классы несбалансированы, так как учитывает ложноположительные и ложноотрицательные предсказания.\n",
    "\n",
    "Для задачи регрессии:\n",
    "\n",
    "1) Mean Absolute Error (MAE): средняя абсолютная ошибка, показывает среднее отклонение предсказаний от фактических значений и хорошо интерпретируется.\n",
    "2) Mean Squared Error (MSE): средняя квадратичная ошибка, показывает более крупные ошибки больше чем MAE, что делает его полезным для выявления больших отклонений в прогнозах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Создание бейзлайна и оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Обучить модели из sklearn (для классификации) для выбранных наборов данных и оценить качество моделей (для классификации) по выбранным метрикам на выбранных наборах данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предобработаем данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение и очистка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list()\n",
    "with open('UCI HAR Dataset/features.txt') as f:\n",
    "    features = [line.split()[1] for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дубликаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen = set()\n",
    "uniq_features = []\n",
    "for idx, x in enumerate(features):\n",
    "    if x not in seen:\n",
    "        uniq_features.append(x)\n",
    "        seen.add(x)\n",
    "    elif x + 'n' not in seen:\n",
    "        uniq_features.append(x + 'n')\n",
    "        seen.add(x + 'n')\n",
    "    else:\n",
    "        uniq_features.append(x + 'nn')\n",
    "        seen.add(x + 'nn')\n",
    "len(uniq_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим обучающую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(arg):\n",
    "    if arg == 1: \n",
    "        return 'WALKING'\n",
    "    if arg == 2:\n",
    "        return 'WALKING_UPSTAIRS'\n",
    "    if arg == 3:\n",
    "        return 'WALKING_DOWNSTAIRS'\n",
    "    if arg == 4:\n",
    "        return 'SITTING'\n",
    "    if arg == 5:\n",
    "        return 'STANDING'\n",
    "    if arg == 6:\n",
    "        return 'LAYING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3107987138.py:1: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  X_train = pd.read_csv('UCI HAR Dataset/train/X_train.txt', delim_whitespace=True, header=None, names=uniq_features)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "      <th>ActivityName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>0.280274</td>\n",
       "      <td>-0.012609</td>\n",
       "      <td>-0.1072</td>\n",
       "      <td>-0.998698</td>\n",
       "      <td>-0.984339</td>\n",
       "      <td>-0.992663</td>\n",
       "      <td>-0.99929</td>\n",
       "      <td>-0.982526</td>\n",
       "      <td>-0.992022</td>\n",
       "      <td>-0.941483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083526</td>\n",
       "      <td>-0.139677</td>\n",
       "      <td>0.109975</td>\n",
       "      <td>-0.04667</td>\n",
       "      <td>-0.63593</td>\n",
       "      <td>0.110069</td>\n",
       "      <td>0.280759</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>SITTING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  \\\n",
       "3312           0.280274          -0.012609            -0.1072   \n",
       "\n",
       "      tBodyAcc-std()-X  tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  \\\n",
       "3312         -0.998698         -0.984339         -0.992663          -0.99929   \n",
       "\n",
       "      tBodyAcc-mad()-Y  tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  \\\n",
       "3312         -0.982526         -0.992022         -0.941483  ...   \n",
       "\n",
       "      angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "3312                    -0.083526                             -0.139677   \n",
       "\n",
       "      angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "3312                          0.109975                              -0.04667   \n",
       "\n",
       "      angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  \\\n",
       "3312              -0.63593              0.110069              0.280759   \n",
       "\n",
       "      subject  Activity  ActivityName  \n",
       "3312       17         4       SITTING  \n",
       "\n",
       "[1 rows x 564 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv('UCI HAR Dataset/train/X_train.txt', delim_whitespace=True, header=None, names=uniq_features)\n",
    "X_train['subject'] = pd.read_csv('UCI HAR Dataset/train/subject_train.txt', header=None)\n",
    "\n",
    "y_train = pd.read_csv('UCI HAR Dataset/train/y_train.txt', names=['Activity'])\n",
    "y_train_labels = y_train.map(map_func)\n",
    "\n",
    "train = X_train\n",
    "train['Activity'] = y_train\n",
    "train['ActivityName'] = y_train_labels\n",
    "train.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/2096605161.py:1: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  X_test = pd.read_csv('UCI HAR Dataset/test/X_test.txt', delim_whitespace=True, header=None, names=uniq_features)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "      <th>ActivityName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>0.237046</td>\n",
       "      <td>-0.099672</td>\n",
       "      <td>-0.057156</td>\n",
       "      <td>-0.928608</td>\n",
       "      <td>-0.11664</td>\n",
       "      <td>-0.621681</td>\n",
       "      <td>-0.932908</td>\n",
       "      <td>-0.114518</td>\n",
       "      <td>-0.615301</td>\n",
       "      <td>-0.927723</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275683</td>\n",
       "      <td>0.289203</td>\n",
       "      <td>0.089517</td>\n",
       "      <td>-0.016266</td>\n",
       "      <td>-0.368489</td>\n",
       "      <td>0.468678</td>\n",
       "      <td>-0.198403</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>SITTING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  \\\n",
       "1075           0.237046          -0.099672          -0.057156   \n",
       "\n",
       "      tBodyAcc-std()-X  tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  \\\n",
       "1075         -0.928608          -0.11664         -0.621681         -0.932908   \n",
       "\n",
       "      tBodyAcc-mad()-Y  tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  \\\n",
       "1075         -0.114518         -0.615301         -0.927723  ...   \n",
       "\n",
       "      angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "1075                    -0.275683                              0.289203   \n",
       "\n",
       "      angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "1075                          0.089517                             -0.016266   \n",
       "\n",
       "      angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  \\\n",
       "1075             -0.368489              0.468678             -0.198403   \n",
       "\n",
       "      subject  Activity  ActivityName  \n",
       "1075       10         4       SITTING  \n",
       "\n",
       "[1 rows x 564 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv('UCI HAR Dataset/test/X_test.txt', delim_whitespace=True, header=None, names=uniq_features)\n",
    "X_test['subject'] = pd.read_csv('UCI HAR Dataset/test/subject_test.txt', header=None)\n",
    "\n",
    "y_test = pd.read_csv('UCI HAR Dataset/test/y_test.txt', names=['Activity'])\n",
    "y_test_labels = y_test.map(map_func)\n",
    "\n",
    "test = X_test\n",
    "test['Activity'] = y_test\n",
    "test['ActivityName'] = y_test_labels\n",
    "test.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поменяем названия функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z',\n",
       "       'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z',\n",
       "       'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z',\n",
       "       'tBodyAcc-max()-X',\n",
       "       ...\n",
       "       'angle(tBodyAccMean,gravity)', 'angle(tBodyAccJerkMean),gravityMean)',\n",
       "       'angle(tBodyGyroMean,gravityMean)',\n",
       "       'angle(tBodyGyroJerkMean,gravityMean)', 'angle(X,gravityMean)',\n",
       "       'angle(Y,gravityMean)', 'angle(Z,gravityMean)', 'subject', 'Activity',\n",
       "       'ActivityName'],\n",
       "      dtype='object', length=564)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = train.columns\n",
    "\n",
    "columns = columns.str.replace('[()]','')\n",
    "columns = columns.str.replace('[-]', '')\n",
    "columns = columns.str.replace('[,]','')\n",
    "\n",
    "train.columns = columns\n",
    "test.columns = columns\n",
    "\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним выборки как csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/train.csv', index=False)\n",
    "test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Обучить модели логистической регрессии из sklearn (для классификации) для выбранных наборов данных и оценить качество моделей (для классификации) по выбранным метрикам на выбранных наборах данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9586019681031558\n",
      "Recall = 0.9586019681031558\n",
      "Precision = 0.9597941694255984\n",
      "F1 Score = 0.9584674380821284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.utils\n",
    "\n",
    "def read_data(file):\n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    data = sklearn.utils.shuffle(data)\n",
    "    \n",
    "    X_data = data.drop(['subject', 'Activity', 'ActivityName'], axis=1)\n",
    "    y_data = data.ActivityName\n",
    "    \n",
    "    return np.array(X_data), np.array(y_data)\n",
    "\n",
    "def train_model(train_x, train_y, validation=None):\n",
    "    model = LogisticRegression()\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    \n",
    "    if validation is not None:\n",
    "        y_hat = model.predict(validation[0])\n",
    "        acc = metrics.accuracy_score(validation[1], y_hat)\n",
    "        print(f\"Accuracy = {acc}\")\n",
    "        recall = metrics.recall_score(validation[1], y_hat, average='weighted')\n",
    "        precision = metrics.precision_score(validation[1], y_hat, average='weighted')\n",
    "        f1 = metrics.f1_score(validation[1], y_hat, average='weighted')\n",
    "        print(f\"Recall = {recall}\")\n",
    "        print(f\"Precision = {precision}\")\n",
    "        print(f\"F1 Score = {f1}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "train_X, train_y = read_data('data/train.csv')\n",
    "test_X, test_y = read_data('data/test.csv')\n",
    "\n",
    "model = train_model(train_X, train_y, validation=(test_X, test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Обучить модели из sklearn (для регрессии) для выбранных наборов данных и оценить качество моделей (для регрессии) по выбранным метрикам на выбранных наборах данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (CO2 Emissions): 13.13\n",
      "MSE (CO2 Emissions): 377.79\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]]\n",
    "y = data['CO2 Emissions(g/km)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linear_regressor.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE (CO2 Emissions): {mae:.2f}\")\n",
    "print(f\"MSE (CO2 Emissions): {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Улучшение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Сформулировать гипотезы (препроцессинг данных, визуализация данных, формирование новых признаков, подбор гиперпараметров на кросс-валидации и т.д.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Формирование новых признаков\n",
    "    - Комбинация признаков: Например, для задачи регрессии по CO2 выбросам можно создать новый признак, комбинирующий объем двигателя и расход топлива, или вывести дополнительные признаки, отражающие производительность.\n",
    "\n",
    "2.  Подбор гиперпараметров\n",
    "    - Для логистической регрессии:\n",
    "\n",
    "        - C (инверсия регуляризации): Найти оптимальный уровень регуляризации. Маленький C — сильная регуляризация, большой C — слабая.\n",
    "        - solver: Подобрать лучший метод оптимизации (например, 'lbfgs', 'liblinear', 'saga' и т.д.) для конкретных данных.\n",
    "\n",
    "    - Для линейной регрессии:\n",
    "\n",
    "        - alpha: Параметр регуляризации для методов Ridge и Lasso.\n",
    "        - Проверить влияние применения регуляризованных моделей:\n",
    "            - Ridge (L2-регуляризация).\n",
    "            - Lasso (L1-регуляризация).\n",
    "\n",
    "3. Методы оптимизации\n",
    "    - Grid Search: Провести поиск по сетке (Grid Search) для поиска оптимальных параметров модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей, оценка качества обучения моделей по метрикам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Масштабируем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "test_X_scaled = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подбор гиперпараметров и Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия (классификация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для логистической регрессии (HAR): {'C': 1, 'max_iter': 1000, 'solver': 'liblinear'}\n",
      "Лучший результат на кросс-валидации (HAR): 0.9849009188991709\n",
      "Accuracy: 0.9613165931455717\n",
      "Precision: 0.963249239599378\n",
      "Recall: 0.9613165931455717\n",
      "F1 Score: 0.9613709750318309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "param_grid_logreg = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'max_iter': [1000, 5000, 10000],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "}\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "grid_search_logreg = GridSearchCV(logreg, param_grid_logreg, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_logreg.fit(train_X_scaled, train_y)\n",
    "best_logreg = grid_search_logreg.best_estimator_\n",
    "\n",
    "print(\"Лучшие параметры для логистической регрессии (HAR):\", grid_search_logreg.best_params_)\n",
    "print(\"Лучший результат на кросс-валидации (HAR):\", grid_search_logreg.best_score_)\n",
    "\n",
    "y_pred_logreg = best_logreg.predict(test_X_scaled)\n",
    "accuracy = accuracy_score(test_y, y_pred_logreg)\n",
    "precision = precision_score(test_y, y_pred_logreg, average='weighted')\n",
    "recall = recall_score(test_y, y_pred_logreg, average='weighted')\n",
    "f1 = f1_score(test_y, y_pred_logreg, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Линейная регрессия (регрессия)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для Ridge регрессии (CO2): {'alpha': 10, 'solver': 'lsqr'}\n",
      "Лучший результат на кросс-валидации (CO2): 375.35785425391526\n",
      "MAE (CO2 Emissions): 13.12377206913496\n",
      "MSE (CO2 Emissions): 380.5580689476625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.332e+04, tolerance: 1.443e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для Lasso регрессии (CO2): {'alpha': 0.01, 'max_iter': 1000}\n",
      "Лучший результат на кросс-валидации (CO2): 375.7968494885642\n",
      "MAE (CO2 Emissions): 13.146135034120853\n",
      "MSE (CO2 Emissions): 378.50093303746394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'saga']\n",
    "}\n",
    "\n",
    "ridge = Ridge()\n",
    "grid_search_ridge = GridSearchCV(ridge, param_grid_ridge, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "best_ridge = grid_search_ridge.best_estimator_\n",
    "\n",
    "print(\"Лучшие параметры для Ridge регрессии (CO2):\", grid_search_ridge.best_params_)\n",
    "print(\"Лучший результат на кросс-валидации (CO2):\", -grid_search_ridge.best_score_)\n",
    "\n",
    "y_pred_ridge = best_ridge.predict(X_test)\n",
    "print(\"MAE (CO2 Emissions):\", mean_absolute_error(y_test, y_pred_ridge))\n",
    "print(\"MSE (CO2 Emissions):\", mean_squared_error(y_test, y_pred_ridge))\n",
    "\n",
    "param_grid_lasso = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter': [1000, 5000, 10000]\n",
    "}\n",
    "\n",
    "lasso = Lasso()\n",
    "grid_search_lasso = GridSearchCV(lasso, param_grid_lasso, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_lasso.fit(X_train, y_train)\n",
    "best_lasso = grid_search_lasso.best_estimator_\n",
    "\n",
    "print(\"Лучшие параметры для Lasso регрессии (CO2):\", grid_search_lasso.best_params_)\n",
    "print(\"Лучший результат на кросс-валидации (CO2):\", -grid_search_lasso.best_score_)\n",
    "\n",
    "y_pred_lasso = best_lasso.predict(X_test)\n",
    "print(\"MAE (CO2 Emissions):\", mean_absolute_error(y_test, y_pred_lasso))\n",
    "print(\"MSE (CO2 Emissions):\", mean_squared_error(y_test, y_pred_lasso))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Добавление нового признака (для задачи регрессии)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для Ridge: {'alpha': 0.1, 'solver': 'svd'}\n",
      "Ridge MAE: 7.29\n",
      "Ridge MSE: 139.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.221e+03, tolerance: 1.421e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.627e+03, tolerance: 1.434e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.816e+03, tolerance: 1.409e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.495e+03, tolerance: 1.409e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для Lasso: {'alpha': 0.01, 'max_iter': 20000}\n",
      "Lasso MAE: 7.45\n",
      "Lasso MSE: 147.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.905e+03, tolerance: 1.779e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]]\n",
    "y = data['CO2 Emissions(g/km)']\n",
    "\n",
    "X_train_new_field, X_test_new_field, y_train_new_field, y_test_new_field = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_new_field['Engine_Fuel'] = X_train_new_field['Engine Size(L)'] * X_train_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "X_test_new_field['Engine_Fuel'] = X_test_new_field['Engine Size(L)'] * X_test_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_new_field)\n",
    "X_test_poly = poly.transform(X_test_new_field)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)\n",
    "\n",
    "ridge = Ridge(max_iter=20000)\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'saga']\n",
    "}\n",
    "grid_search_ridge = GridSearchCV(ridge, param_grid_ridge, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_ridge.fit(X_train_poly_scaled, y_train_new_field)\n",
    "best_ridge = grid_search_ridge.best_estimator_\n",
    "\n",
    "y_pred_ridge = best_ridge.predict(X_test_poly_scaled)\n",
    "ridge_mae = mean_absolute_error(y_test_new_field, y_pred_ridge)\n",
    "ridge_mse = mean_squared_error(y_test_new_field, y_pred_ridge)\n",
    "\n",
    "print(\"Лучшие параметры для Ridge:\", grid_search_ridge.best_params_)\n",
    "print(f\"Ridge MAE: {ridge_mae:.2f}\")\n",
    "print(f\"Ridge MSE: {ridge_mse:.2f}\")\n",
    "\n",
    "lasso = Lasso(max_iter=20000)\n",
    "param_grid_lasso = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'max_iter': [20000]\n",
    "}\n",
    "grid_search_lasso = GridSearchCV(lasso, param_grid_lasso, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_lasso.fit(X_train_poly_scaled, y_train_new_field)\n",
    "best_lasso = grid_search_lasso.best_estimator_\n",
    "\n",
    "y_pred_lasso = best_lasso.predict(X_test_poly_scaled)\n",
    "lasso_mae = mean_absolute_error(y_test_new_field, y_pred_lasso)\n",
    "lasso_mse = mean_squared_error(y_test_new_field, y_pred_lasso)\n",
    "\n",
    "print(\"Лучшие параметры для Lasso:\", grid_search_lasso.best_params_)\n",
    "print(f\"Lasso MAE: {lasso_mae:.2f}\")\n",
    "print(f\"Lasso MSE: {lasso_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Окончательный улучшенный бейзлайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие параметры для классификации:\n",
    "{'C': 1, 'max_iter': 1000, 'solver': 'liblinear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9619952494061758\n",
      "Recall = 0.9619952494061758\n",
      "Precision = 0.9632384391448742\n",
      "F1 Score = 0.9618652634530593\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.utils\n",
    "\n",
    "def read_data(file):\n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    data = sklearn.utils.shuffle(data)\n",
    "    \n",
    "    X_data = data.drop(['subject', 'Activity', 'ActivityName'], axis=1)\n",
    "    y_data = data.ActivityName\n",
    "    \n",
    "    return np.array(X_data), np.array(y_data)\n",
    "\n",
    "def train_model(train_x, train_y, validation=None):\n",
    "    model = LogisticRegression(max_iter=1000, solver='liblinear', multi_class='ovr', C = 1)\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    \n",
    "    if validation is not None:\n",
    "        y_hat = model.predict(validation[0])\n",
    "        acc = metrics.accuracy_score(validation[1], y_hat)\n",
    "        print(f\"Accuracy = {acc}\")\n",
    "        recall = metrics.recall_score(validation[1], y_hat, average='weighted')\n",
    "        precision = metrics.precision_score(validation[1], y_hat, average='weighted')\n",
    "        f1 = metrics.f1_score(validation[1], y_hat, average='weighted')\n",
    "        print(f\"Recall = {recall}\")\n",
    "        print(f\"Precision = {precision}\")\n",
    "        print(f\"F1 Score = {f1}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "train_X, train_y = read_data('data/train.csv')\n",
    "test_X, test_y = read_data('data/test.csv')\n",
    "\n",
    "model = train_model(train_X, train_y, validation=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более лучшие (по MAE, по кросс-валидации и по метрикам после добавления нового признака) для задачи регрессии были показаны моделью Ridge, поэтому улучшенный бейзлайн смормируем на основе модели Ridge с гиперпараметрами:\n",
    "{'alpha': 0.1, 'solver': 'svd'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 7.29\n",
      "MSE: 139.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]]\n",
    "y = data['CO2 Emissions(g/km)']\n",
    "\n",
    "X_train_new_field, X_test_new_field, y_train_new_field, y_test_new_field = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_new_field['Engine_Fuel'] = X_train_new_field['Engine Size(L)'] * X_train_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "X_test_new_field['Engine_Fuel'] = X_test_new_field['Engine Size(L)'] * X_test_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_new_field)\n",
    "X_test_poly = poly.transform(X_test_new_field)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)\n",
    "\n",
    "ridge = Ridge(max_iter=20000, alpha=0.1, solver='svd')\n",
    "ridge.fit(X_train_poly_scaled, y_train_new_field)\n",
    "\n",
    "y_pred_ridge = ridge.predict(X_test_poly_scaled)\n",
    "ridge_mae = mean_absolute_error(y_test_new_field, y_pred_ridge)\n",
    "ridge_mse = mean_squared_error(y_test_new_field, y_pred_ridge)\n",
    "\n",
    "print(f\"MAE: {ridge_mae:.2f}\")\n",
    "print(f\"MSE: {ridge_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче классификации на наборе данных HAR модель логистической регрессии без оптимизаций продемонстрировала хорошие результаты с точностью около 96%. Однако дальнейшая оптимизация с помощью Grid Search и Random Search позволила немного улучшить результаты, достигнув точности в 96.13% и F1-меры 0.96, что подтвердило важность настройки гиперпараметров для улучшения классификации.\n",
    "\n",
    "В задаче регрессии для прогноза выбросов CO2 модель линейной регрессии без улучшений показала значения MAE и MSE около 13.13 и 377.79 соответственно. После подбора гиперпараметров для Ridge и Lasso моделей ошибки немного снизились, но наибольшее улучшение было достигнуто при добавлении нового признака Engine_Fuel, который позволил существенно уменьшить MAE и MSE, снизив их до 7.29 и 139.02 для Ridge.\n",
    "\n",
    "Комбинирование настройки гиперпараметров и создания новых признаков значительно улучшило результаты моделей, особенно в задаче регрессии, где добавление информативных признаков дало наибольшее улучшение. Эти подходы подтверждают свою важность для повышения качества моделей в машинном обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Имплементация алгоритма машинного обучения "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация логистической регрессии для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LogisticRegressionCustom:\n",
    "    def __init__(self, lr=0.01, max_iter=1000, penalty=None, C=1.0, tol=1e-4):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.penalty = penalty\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "        self.y_train = y_train\n",
    "        self.weights = np.zeros(self.X_train.shape[1])\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            predictions = self._sigmoid(np.dot(self.X_train, self.weights))\n",
    "            errors = predictions - self.y_train\n",
    "            gradient = np.dot(self.X_train.T, errors) / len(y_train)\n",
    "            \n",
    "            if self.penalty == 'l2':\n",
    "                gradient += (1 / self.C) * self.weights\n",
    "            elif self.penalty == 'l1':\n",
    "                gradient += (1 / self.C) * np.sign(self.weights)\n",
    "            \n",
    "            self.weights -= self.lr * gradient\n",
    "            \n",
    "            if np.linalg.norm(gradient) < self.tol:\n",
    "                break\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "\n",
    "        return self._sigmoid(np.dot(X_test, self.weights))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        probabilities = self.predict_proba(X_test)\n",
    "\n",
    "        return (probabilities >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация линейной регрессии для регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionCustom:\n",
    "    def __init__(self, lr=0.01, max_iter=1000, penalty=None, alpha=0.0, tol=1e-4):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.penalty = penalty\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "        self.y_train = y_train\n",
    "        self.weights = np.zeros(self.X_train.shape[1])\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            predictions = np.dot(self.X_train, self.weights)\n",
    "            errors = predictions - self.y_train\n",
    "            gradient = np.dot(self.X_train.T, errors) / len(y_train)\n",
    "            \n",
    "            if self.penalty == 'l2':\n",
    "                gradient += (2 * self.alpha) * self.weights\n",
    "            elif self.penalty == 'l1':\n",
    "                gradient += self.alpha * np.sign(self.weights)\n",
    "            \n",
    "            self.weights -= self.lr * gradient\n",
    "            \n",
    "            if np.linalg.norm(gradient) < self.tol:\n",
    "                break\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "        \n",
    "        return np.dot(X_test, self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей на выбранных датасетах и вывод метрик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.18900576857821513\n",
      "Recall = 0.18900576857821513\n",
      "Precision = 0.07599169489560975\n",
      "F1 Score = 0.10811883566310325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def normalize_data(X):\n",
    "    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "def encode_labels(y):\n",
    "    label_to_num = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "    num_to_label = {idx: label for label, idx in label_to_num.items()}\n",
    "    y_encoded = np.array([label_to_num[label] for label in y])\n",
    "    return y_encoded, label_to_num, num_to_label\n",
    "\n",
    "def train_custom_logistic_regression(train_x, train_y, validation=None):\n",
    "    train_x_normalized = normalize_data(train_x)\n",
    "    train_y_encoded, _, _ = encode_labels(train_y)\n",
    "\n",
    "    if validation is not None:\n",
    "        validation_x_normalized = normalize_data(validation[0])\n",
    "        validation_y_encoded, _, _ = encode_labels(validation[1])\n",
    "    else:\n",
    "        validation_x_normalized = None\n",
    "        validation_y_encoded = None\n",
    "\n",
    "    model = LogisticRegressionCustom(lr=0.01, max_iter=2000, penalty='l1')\n",
    "    model.fit(train_x_normalized, train_y_encoded)\n",
    "\n",
    "    y_hat = model.predict(validation_x_normalized)\n",
    "    acc = metrics.accuracy_score(validation_y_encoded, y_hat)\n",
    "    print(f\"Accuracy = {acc}\")\n",
    "    recall = metrics.recall_score(validation_y_encoded, y_hat, average='weighted')\n",
    "    precision = metrics.precision_score(validation_y_encoded, y_hat, average='weighted')\n",
    "    f1 = metrics.f1_score(validation_y_encoded, y_hat, average='weighted')\n",
    "    print(f\"Recall = {recall}\")\n",
    "    print(f\"Precision = {precision}\")\n",
    "    print(f\"F1 Score = {f1}\")\n",
    "\n",
    "train_X, train_y = read_data('data/train.csv')\n",
    "test_X, test_y = read_data('data/test.csv')\n",
    "\n",
    "train_custom_logistic_regression(train_X, train_y, validation=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 13.18\n",
      "MSE: 384.77\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]].values.astype(np.float64)\n",
    "y = data['CO2 Emissions(g/km)'].values.astype(np.float64)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "custom_linear_regressor = LinearRegressionCustom(lr=0.01, max_iter=1000)\n",
    "custom_linear_regressor.fit(X_train, y_train)\n",
    "\n",
    "predictions = custom_linear_regressor.predict(X_test)\n",
    "\n",
    "mae = np.mean(np.abs(predictions - y_test))\n",
    "mse = np.mean((predictions - y_test) ** 2)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение результатов с п.2. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты сравнения моделей показывают, что самописные реализации логистической и линейной регрессий уступают по качеству моделям из библиотеки sklearn.\n",
    "\n",
    "В задаче классификации модели из sklearn продемонстрировали высокую точность (Accuracy = 95.86%), а также сбалансированные значения полноты (Recall = 96.17%), точности (Precision = 95.97%) и F1-метрики (F1 Score = 95.84%). В то же время, самописная модель показала крайне низкие результаты: точность составила всего 18.29%, а значения других метрик, таких как полнота, точность и F1, также оказались крайне низкими, что указывает на ее неспособность правильно классифицировать данные. Это может быть связано с проблемой в датесете.\n",
    "\n",
    "В задаче регрессии разрыв между моделями менее выражен, однако модели из sklearn также показали лучшее качество. Средняя абсолютная ошибка (MAE) для sklearn составила 13.13, а среднеквадратичная ошибка (MSE) — 377.79, что немного лучше результатов самописной модели (MAE = 13.18, MSE = 384.77). Это указывает на то, что самописная линейная регрессия работает корректно, но все же не достигает точности и оптимальности, предложенной готовыми библиотечными реализациями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Добавление нового признака (для задачи регрессии)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 9.93\n",
      "MSE: 247.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]]\n",
    "y = data['CO2 Emissions(g/km)']\n",
    "\n",
    "X_train_new_field, X_test_new_field, y_train_new_field, y_test_new_field = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_new_field['Engine_Fuel'] = X_train_new_field['Engine Size(L)'] * X_train_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "X_test_new_field['Engine_Fuel'] = X_test_new_field['Engine Size(L)'] * X_test_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_new_field)\n",
    "X_test_poly = poly.transform(X_test_new_field)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)\n",
    "\n",
    "model = LinearRegressionCustom(lr=0.01, max_iter=2000)\n",
    "\n",
    "model.fit(X_train_poly_scaled, y_train_new_field)\n",
    "\n",
    "y_pred_test = model.predict(X_test_poly_scaled)\n",
    "\n",
    "test_mae = np.mean(np.abs(y_pred_test - y_test_new_field))\n",
    "test_mse = np.mean((y_pred_test - y_test_new_field) ** 2)\n",
    "\n",
    "print(f\"MAE: {test_mae:.2f}\")\n",
    "print(f\"MSE: {test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор гиперпараметров и Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для LinearRegressionCustom: {'lr': 0.1, 'max_iter': 10000}\n",
      "Лучший результат на валидационной выборке (MSE): 377.9191266873952\n",
      "MAE: 13.14\n",
      "MSE: 377.92\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]].values.astype(np.float64)\n",
    "y = data['CO2 Emissions(g/km)'].values.astype(np.float64)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "param_grid_linear = {\n",
    "    'lr': [0.001, 0.01, 0.1],\n",
    "    'max_iter': [1000, 2000, 5000, 10000]\n",
    "}\n",
    "\n",
    "best_params = None\n",
    "best_score = float('inf')\n",
    "results = []\n",
    "\n",
    "for lr in param_grid_linear['lr']:\n",
    "    for max_iter in param_grid_linear['max_iter']:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        model = LinearRegressionCustom(lr=lr, max_iter=max_iter)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred_val = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred_val)\n",
    "        \n",
    "        results.append({'lr': lr, 'max_iter': max_iter, 'mse': mse})\n",
    "        \n",
    "        if mse < best_score:\n",
    "            best_score = mse\n",
    "            best_params = {'lr': lr, 'max_iter': max_iter}\n",
    "\n",
    "best_linear_model = LinearRegressionCustom(lr=best_params['lr'], max_iter=best_params['max_iter'])\n",
    "best_linear_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = best_linear_model.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"Лучшие параметры для LinearRegressionCustom:\", best_params)\n",
    "print(\"Лучший результат на валидационной выборке (MSE):\", best_score)\n",
    "print(f\"MAE: {test_mae:.2f}\")\n",
    "print(f\"MSE: {test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:26: RuntimeWarning: overflow encountered in multiply\n",
      "  gradient += (1 / self.C) * self.weights\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:26: RuntimeWarning: overflow encountered in multiply\n",
      "  gradient += (1 / self.C) * self.weights\n",
      "/var/folders/kc/ml55mx4s5fl9fqwbt_5gpdz49kkcmp/T/ipykernel_1182/3672269993.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'lr': 0.5, 'max_iter': 500, 'penalty': 'l1', 'C': 1.0}\n",
      "Accuracy: 0.1965\n",
      "Recall: 0.1965\n",
      "Precision: 0.0831\n",
      "F1 Score: 0.1160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "def normalize_data(X):\n",
    "    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "def encode_labels(y):\n",
    "    label_to_num = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "    num_to_label = {idx: label for label, idx in label_to_num.items()}\n",
    "    y_encoded = np.array([label_to_num[label] for label in y])\n",
    "    return y_encoded, label_to_num, num_to_label\n",
    "\n",
    "def read_data(filepath):\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv(filepath)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    return X, y\n",
    "\n",
    "def optimize_hyperparameters(train_X, train_y, test_X, test_y):\n",
    "    train_X_normalized = normalize_data(train_X)\n",
    "    train_y_encoded, _, _ = encode_labels(train_y)\n",
    "    test_X_normalized = normalize_data(test_X)\n",
    "    test_y_encoded, _, _ = encode_labels(test_y)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [0.001, 0.01, 0.1, 0.5],\n",
    "        'max_iter': [500, 1000, 2000],\n",
    "        'penalty': [None, 'l1', 'l2'],\n",
    "        'C': [0.1, 1.0, 10.0]\n",
    "    }\n",
    "\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "    results = []\n",
    "\n",
    "    for lr in param_grid['lr']:\n",
    "        for max_iter in param_grid['max_iter']:\n",
    "            for penalty in param_grid['penalty']:\n",
    "                for C in param_grid['C']:\n",
    "                    if penalty is None and C != 1.0:\n",
    "                        continue\n",
    "\n",
    "                    model = LogisticRegressionCustom(lr=lr, max_iter=max_iter, penalty=penalty, C=C)\n",
    "                    model.fit(train_X_normalized, train_y_encoded)\n",
    "\n",
    "                    y_hat = model.predict(test_X_normalized)\n",
    "                    acc = accuracy_score(test_y_encoded, y_hat)\n",
    "                    f1 = f1_score(test_y_encoded, y_hat, average='weighted')\n",
    "\n",
    "                    results.append({'lr': lr, 'max_iter': max_iter, 'penalty': penalty, 'C': C, 'accuracy': acc, 'f1': f1})\n",
    "\n",
    "                    if f1 > best_score:\n",
    "                        best_score = f1\n",
    "                        best_params = {'lr': lr, 'max_iter': max_iter, 'penalty': penalty, 'C': C}\n",
    "\n",
    "    best_model = LogisticRegressionCustom(\n",
    "        lr=best_params['lr'],\n",
    "        max_iter=best_params['max_iter'],\n",
    "        penalty=best_params['penalty'],\n",
    "        C=best_params['C']\n",
    "    )\n",
    "    best_model.fit(train_X_normalized, train_y_encoded)\n",
    "    y_pred = best_model.predict(test_X_normalized)\n",
    "\n",
    "    acc = accuracy_score(test_y_encoded, y_pred)\n",
    "    recall = recall_score(test_y_encoded, y_pred, average='weighted')\n",
    "    precision = precision_score(test_y_encoded, y_pred, average='weighted')\n",
    "    f1 = f1_score(test_y_encoded, y_pred, average='weighted')\n",
    "\n",
    "    print(\"Лучшие параметры:\", best_params)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return best_params, results\n",
    "\n",
    "train_X, train_y = read_data('data/train.csv')\n",
    "test_X, test_y = read_data('data/test.csv')\n",
    "\n",
    "best_params, results = optimize_hyperparameters(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Окончательный улучшенный бейзлайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие параметры для классификации:\n",
    "{'lr': 0.5, 'max_iter': 500, 'penalty': 'l1', 'C': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.19579233118425518\n",
      "Recall = 0.19579233118425518\n",
      "Precision = 0.08286520400450481\n",
      "F1 Score = 0.11559601211525375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daustinov/study/multimedia/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def normalize_data(X):\n",
    "    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "def encode_labels(y):\n",
    "    label_to_num = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "    num_to_label = {idx: label for label, idx in label_to_num.items()}\n",
    "    y_encoded = np.array([label_to_num[label] for label in y])\n",
    "    return y_encoded, label_to_num, num_to_label\n",
    "\n",
    "def train_custom_logistic_regression(train_x, train_y, validation=None):\n",
    "    train_x_normalized = normalize_data(train_x)\n",
    "    train_y_encoded, _, _ = encode_labels(train_y)\n",
    "\n",
    "    if validation is not None:\n",
    "        validation_x_normalized = normalize_data(validation[0])\n",
    "        validation_y_encoded, _, _ = encode_labels(validation[1])\n",
    "    else:\n",
    "        validation_x_normalized = None\n",
    "        validation_y_encoded = None\n",
    "\n",
    "    model = LogisticRegressionCustom(lr=0.5, max_iter=500, penalty='l1', C = 1.0)\n",
    "    model.fit(train_x_normalized, train_y_encoded)\n",
    "\n",
    "    y_hat = model.predict(validation_x_normalized)\n",
    "    acc = metrics.accuracy_score(validation_y_encoded, y_hat)\n",
    "    print(f\"Accuracy = {acc}\")\n",
    "    recall = metrics.recall_score(validation_y_encoded, y_hat, average='weighted')\n",
    "    precision = metrics.precision_score(validation_y_encoded, y_hat, average='weighted')\n",
    "    f1 = metrics.f1_score(validation_y_encoded, y_hat, average='weighted')\n",
    "    print(f\"Recall = {recall}\")\n",
    "    print(f\"Precision = {precision}\")\n",
    "    print(f\"F1 Score = {f1}\")\n",
    "\n",
    "train_X, train_y = read_data('data/train.csv')\n",
    "test_X, test_y = read_data('data/test.csv')\n",
    "\n",
    "train_custom_logistic_regression(train_X, train_y, validation=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие параметры для регрессии:\n",
    "{'lr': 0.1, 'max_iter': 10000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 7.99\n",
      "MSE: 189.17\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"CO2 Emission by Vehicles/CO2 Emissions_Canada.csv\")\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Vehicle Class'], drop_first=True)\n",
    "\n",
    "X = data[['Engine Size(L)', 'Cylinders', 'Fuel Consumption City (L/100 km)', \n",
    "          'Fuel Consumption Hwy (L/100 km)', 'Fuel Consumption Comb (L/100 km)'] + \n",
    "         [col for col in data.columns if 'Vehicle Class_' in col]]\n",
    "y = data['CO2 Emissions(g/km)']\n",
    "\n",
    "X_train_new_field, X_test_new_field, y_train_new_field, y_test_new_field = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_new_field['Engine_Fuel'] = X_train_new_field['Engine Size(L)'] * X_train_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "X_test_new_field['Engine_Fuel'] = X_test_new_field['Engine Size(L)'] * X_test_new_field['Fuel Consumption Comb (L/100 km)']\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_new_field)\n",
    "X_test_poly = poly.transform(X_test_new_field)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)\n",
    "\n",
    "model = LinearRegressionCustom(lr=0.01, max_iter=10000)\n",
    "\n",
    "model.fit(X_train_poly_scaled, y_train_new_field)\n",
    "\n",
    "y_pred_test = model.predict(X_test_poly_scaled)\n",
    "\n",
    "test_mae = np.mean(np.abs(y_pred_test - y_test_new_field))\n",
    "test_mse = np.mean((y_pred_test - y_test_new_field) ** 2)\n",
    "\n",
    "print(f\"MAE: {test_mae:.2f}\")\n",
    "print(f\"MSE: {test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение результатов самописных моделей и моделей из библиотеки sklearn показывает значительное преимущество последних. Для задачи классификации самописная логистическая регрессия, даже после оптимизации гиперпараметров с использованием GridSearch, показала низкие результаты: Accuracy составила всего 0.1965, а F1 Score — 0.1160. В то же время логистическая регрессия из sklearn достигла гораздо более высоких значений. Это можно объяснить отсутствием каких-либо оптимизаций в самописной модели логистической регрессии или отсутствием какой-либо предобработки данных, которая есть в моделе в sklearn.\n",
    "\n",
    "В задаче регрессии самописная линейная регрессия также отстает. После оптимизации гиперпараметров её лучшие показатели составили MAE 7.99 и MSE 189.17, тогда как линейная регрессия из sklearn с добавлением нового признака достигла результатов MAE 7.29 и MSE 139.02.\n",
    "\n",
    "Таким образом, модели из sklearn демонстрируют значительно более высокую точность по сравнению с самописными решениями, благодаря их оптимизациям."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimedia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
